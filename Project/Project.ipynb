{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "V6rOAV9rPz1k"
      ],
      "authorship_tag": "ABX9TyMB/O7NNLbqEwjIhQs+LYV5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wilbertcaine/COMP4211/blob/main/Project/Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zj4luAPGr0c0",
        "outputId": "b4c6613c-2599-4383-dc88-879f6eb03587"
      },
      "source": [
        "# https://drive.google.com/file/d/199mVXHt5cxT68gBXCcucMjtJzCwvc8ud/view?usp=sharing\n",
        "# https://drive.google.com/file/d/1d2hJyvo2Go4FxK7br0exFQFdZ1eCexIA/view?usp=sharing\n",
        "# https://drive.google.com/file/d/1yjsMICWqHdEUhF758ndq0hLO96FrYWkR/view?usp=sharing\n",
        "%cd /content\n",
        "!mkdir -p dataset\n",
        "%cd /content/dataset\n",
        "!gdown --id 199mVXHt5cxT68gBXCcucMjtJzCwvc8ud\n",
        "!gdown --id 1d2hJyvo2Go4FxK7br0exFQFdZ1eCexIA\n",
        "!gdown --id 1yjsMICWqHdEUhF758ndq0hLO96FrYWkR\n",
        "\n",
        "# https://drive.google.com/file/d/1p1NQBepQeY-b1hXe9RJVZ3Rrx6-u_UPJ/view?usp=sharing\n",
        "# https://drive.google.com/file/d/1vYIcELaxAVF2tXX03XK9m4k2727vufvw/view?usp=sharing\n",
        "# !gdown --id 1p1NQBepQeY-b1hXe9RJVZ3Rrx6-u_UPJ\n",
        "# !gdown --id 1vYIcELaxAVF2tXX03XK9m4k2727vufvw\n",
        "\n",
        "%cd ../"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/dataset\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=199mVXHt5cxT68gBXCcucMjtJzCwvc8ud\n",
            "To: /content/dataset/train.csv\n",
            "100% 7.62M/7.62M [00:00<00:00, 24.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1d2hJyvo2Go4FxK7br0exFQFdZ1eCexIA\n",
            "To: /content/dataset/Test_Jan.csv\n",
            "100% 124k/124k [00:00<00:00, 3.91MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1yjsMICWqHdEUhF758ndq0hLO96FrYWkR\n",
            "To: /content/dataset/Predict_Jan.csv\n",
            "100% 20.7k/20.7k [00:00<00:00, 6.05MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1p1NQBepQeY-b1hXe9RJVZ3Rrx6-u_UPJ\n",
            "To: /content/dataset/video59-tool.txt\n",
            "100% 20.5k/20.5k [00:00<00:00, 5.43MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1vYIcELaxAVF2tXX03XK9m4k2727vufvw\n",
            "To: /content/dataset/video78-phase.txt\n",
            "100% 495k/495k [00:00<00:00, 7.71MB/s]\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6rOAV9rPz1k"
      },
      "source": [
        "# Trash"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKtVfm29D-ww",
        "outputId": "9937e231-4972-4a5b-f924-6df95a0e1ce9"
      },
      "source": [
        "df = np.loadtxt(f'dataset/video{str(59).zfill(2)}-tool.txt', skiprows=1)\n",
        "# df = np.delete(df, 0, axis=1)\n",
        "df"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.0000e+00, 0.0000e+00, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00],\n",
              "       [2.5000e+01, 0.0000e+00, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00],\n",
              "       [5.0000e+01, 0.0000e+00, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00],\n",
              "       ...,\n",
              "       [2.6075e+04, 1.0000e+00, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00,\n",
              "        1.0000e+00],\n",
              "       [2.6100e+04, 1.0000e+00, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00,\n",
              "        1.0000e+00],\n",
              "       [2.6125e+04, 1.0000e+00, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00,\n",
              "        1.0000e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov3qSP65QdXN"
      },
      "source": [
        "full_phase = np.array(['Preparation', 'CalotTriangleDissection', 'ClippingCutting', 'GallbladderDissection', 'GallbladderPackaging', 'CleaningCoagulation', 'GallbladderRetraction'])\n",
        "full_phase = np.expand_dims(full_phase, axis=1)\n",
        "for i in range(1,80):\n",
        "    df = np.loadtxt(f'dataset/Cholec80/phase_annotations/video{str(i).zfill(2)}-phase.txt', dtype=str, skiprows=1)\n",
        "    df = np.delete(df, 0, axis=1)\n",
        "    df = np.concatenate((full_phase, df))\n",
        "    df = df.flatten()\n",
        "    df = pd.get_dummies(df)\n",
        "    df = np.array(df)\n",
        "    df = np.delete(df, range(0,7), axis=0)\n",
        "    if df.shape[1] != 7:\n",
        "        print(df.shape[1]) \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69GvAjYSUL5H",
        "outputId": "37fa423a-a6b2-4e5c-a469-2f814ea825fa"
      },
      "source": [
        "np.expand_dims(full_phase, axis=1)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['Preparation'],\n",
              "       ['CalotTriangleDissection'],\n",
              "       ['ClippingCutting'],\n",
              "       ['GallbladderDissection'],\n",
              "       ['GallbladderPackaging'],\n",
              "       ['CleaningCoagulation'],\n",
              "       ['GallbladderRetraction']], dtype='<U23')"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1bgvZPVAM3I",
        "outputId": "6e94ea00-d04c-45e0-af2f-1db23895a7b0"
      },
      "source": [
        "full_phase = np.array(['Preparation', 'CalotTriangleDissection', 'ClippingCutting', 'GallbladderDissection', 'GallbladderPackaging', 'CleaningCoagulation', 'GallbladderRetraction'])\n",
        "full_phase = np.expand_dims(full_phase, axis=1)\n",
        "df = np.loadtxt(f'dataset/video{str(78).zfill(2)}-phase.txt', dtype=str, skiprows=1)\n",
        "df = np.delete(df, 0, axis=1)\n",
        "df = np.concatenate((full_phase, df))\n",
        "df = df.flatten()\n",
        "df = pd.get_dummies(df)\n",
        "df = np.array(df)\n",
        "df = np.delete(df, range(0,7), axis=0)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 0 1]\n",
            " ...\n",
            " [0 0 0 ... 0 1 0]\n",
            " [0 0 0 ... 0 1 0]\n",
            " [0 0 0 ... 0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s_xJwA-bbcH"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rx17aneeXjoJ"
      },
      "source": [
        "df = pd.read_csv('dataset/train.csv')\n",
        "df = df.drop(columns=\"datetime\")\n",
        "# df['datetime']=pd.to_datetime(df['datetime'])\n",
        "# df['week_day']=df['datetime'].dt.dayofweek\n",
        "# df['date']=df['datetime'].dt.day\n",
        "# df['month']=df['datetime'].dt.month\n",
        "# df['hour']=df['datetime'].dt.hour\n",
        "a, b, c = np.split(df, [int(.7*len(df)), int(.85*len(df))])"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CycvLB1-U5Aw",
        "outputId": "c0d3f287-a02b-4e38-a655-106ff36d258c"
      },
      "source": [
        "len(a)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30642"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qacsycRNbddB"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WdaeGZWPlaF"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1yJgcOiuASt"
      },
      "source": [
        "class FC_Net_Dataset(Dataset):\n",
        "    def __init__(self, split, csv_dir='dataset/train.csv'):\n",
        "        \"\"\"\n",
        "        Keyword arguments:\n",
        "        split -- an integer either 0, 1, or 2 which indicates train, validation, or test\n",
        "        \"\"\"\n",
        "        super(FC_Net_Dataset, self).__init__()\n",
        "        df = pd.read_csv(csv_dir)\n",
        "        df = df.drop(columns=\"datetime\")\n",
        "        data = np.split(df, [int(.7*len(df)), int(.85*len(df))])\n",
        "        self.data = np.array(data[split])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx][1:], self.data[idx][0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rl-ZSECJbZkX"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwVtd5j6awL6"
      },
      "source": [
        "class FC_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FC_Net, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(15, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, 8)\n",
        "        self.fc4 = nn.Linear(8, 1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.bn = nn.BatchNorm1d(100, affine=False)\n",
        " \n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.bn(x)\n",
        "\n",
        "        x = self.fc4(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Erhn50AnU9z3"
      },
      "source": [
        "def train_epoch(net, optimizer, loss_fn, dataloader, epoch, writer=None, device='cpu'):\n",
        "    ep_loss = 0.0\n",
        "    num_iter = len(dataloader)\n",
        "    net.train()\n",
        "    grad_max = 100.\n",
        "    for n_iter, (xs, y) in enumerate(tqdm(dataloader)):\n",
        "        curr_iter = epoch * num_iter + n_iter\n",
        "        xs, y = xs.to(device), y.to(device)\n",
        "        pred = net(xs) ## step 1. get output\n",
        "        loss = loss_fn(pred, y) ## step 2. compute loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward() ## step 3. backpropagation\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), grad_max) ## L2-norm gradient clipping\n",
        "        optimizer.step() ## step 4. update model weigth\n",
        "        if writer is not None:\n",
        "            writer.add_scalar('Loss/train', loss.data, curr_iter)\n",
        "        # print(\"\\n[ITER %d] LOSS: %.4f\" % (curr_iter, loss.data))\n",
        "        ep_loss += loss.data\n",
        "    ep_loss /= len(dataloader)\n",
        "    if writer is not None:\n",
        "        writer.add_scalar('EpLoss/train', ep_loss, epoch)\n",
        "    print(\"\\n[EP %d] Train LOSS: %.4f\" % (epoch, ep_loss))\n",
        "    return ep_loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def val_epoch(net, loss_fn, dataloader, epoch, val_set, writer=None, device='cpu'):\n",
        "    ep_loss = 0.0\n",
        "    net.eval()\n",
        "    preds, ys = [], []\n",
        "    for n_iter, (xs, y) in enumerate(tqdm(dataloader)):\n",
        "        xs, y = xs.to(device), y.to(device)\n",
        "        pred = net(xs)\n",
        "        loss = loss_fn(pred, y)\n",
        "        ep_loss += loss.data\n",
        "        preds.append(pred.squeeze())\n",
        "        ys.append(y)\n",
        "    ep_loss /= len(dataloader)\n",
        "    print(\"\\n[EP %d] Val LOSS: %.4f\" % (epoch, ep_loss))\n",
        "\n",
        "    preds = torch.stack(preds).squeeze()\n",
        "    ys = torch.stack(ys).squeeze()\n",
        "    # if (epoch + 1) % 5 == 0:\n",
        "    ## denormalize\n",
        "    preds = preds * val_set.std + val_set.mean\n",
        "    ys = ys * val_set.std + val_set.mean\n",
        "\n",
        "    ## plot the prediction and GT close value\n",
        "    plt.plot(ys.cpu().numpy(), label='GT')\n",
        "    plt.plot(preds.cpu().numpy(), label='pred')\n",
        "    plt.title(\"[Ep %d] GT vs Pred\"%(epoch))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    if writer is not None:\n",
        "        writer.add_scalar('EpLoss/val', ep_loss, epoch)\n",
        "    return ep_loss\n",
        "\n",
        "def train_main(net, train_set, val_set, loss_fn, batch_size, lr, n_epoch, name='net'):\n",
        "    train_loader = data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = data.DataLoader(val_set, batch_size=1)\n",
        "\n",
        "    save_dir = f'drive/MyDrive/COMP4211/{name}_{batch_size}_{lr}_{n_epoch}'\n",
        "    writer = SummaryWriter(log_dir=osp.join(save_dir, 'log'))\n",
        "    device = 'cpu' if not torch.cuda.is_available() else 'cuda'\n",
        "    net = net.to(device)\n",
        "    optimizer = torch.optim.AdamW(net.parameters(), lr=lr)\n",
        "\n",
        "    train_ep_losses, val_ep_losses = [], []\n",
        "    for epoch in range(n_epoch):\n",
        "        train_loss = train_epoch(net, optimizer, loss_fn, train_loader, epoch, writer, device)\n",
        "        val_loss = val_epoch(net, loss_fn, val_loader, epoch, val_set, writer, device)\n",
        "        train_ep_losses.append(train_loss)\n",
        "        val_ep_losses.append(val_loss)\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            os.makedirs(osp.join(save_dir, 'weights'), exist_ok=True)\n",
        "            torch.save(net.state_dict(), osp.join(save_dir, 'weights/ep%d.pth' % (epoch + 1)))\n",
        "    \n",
        "    plt.plot(torch.tensor(train_ep_losses).cpu().numpy(), label='train')\n",
        "    plt.plot(torch.tensor(val_ep_losses).cpu().numpy(), label='val')\n",
        "    plt.xlabel('n_epoch')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZXCY3g9PoCn"
      },
      "source": [
        "train_FC_Net_Dataset = FC_Net_Dataset(split=0)\n",
        "val_FC_Net_Dataset = FC_Net_Dataset(split=1)\n",
        "test_FC_Net_Dataset = FC_Net_Dataset(split=2)\n",
        "\n",
        "fc_net = FC_Net();\n",
        "loss = nn.L1Loss()"
      ],
      "execution_count": 144,
      "outputs": []
    }
  ]
}